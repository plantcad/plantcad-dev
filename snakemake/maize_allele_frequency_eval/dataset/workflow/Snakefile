configfile: "config/config.yaml"

include: "rules/common.smk"
include: "rules/ensembl.smk"


rule all:
    input:
        "results/upload.done",


rule process_variants:
    output:
        "results/variants.parquet",
    run:
        (
            pl.read_parquet(
                config["raw_data_path"],
                columns=["CHR", "POS", "REF", "ALT", "REF_FREQ", "ALT_FREQ"],
            )
            .rename({
                "CHR": "chrom", "POS": "pos", "REF": "ref", "ALT": "alt",
                "REF_FREQ": "ref_count", "ALT_FREQ": "alt_count",
            })
            .with_columns(
                pl.col("chrom").cast(str),
                pl.col("alt_count").alias("AC"),
                (pl.col("ref_count") + pl.col("alt_count")).alias("AN"),
            )
            .drop(["ref_count", "alt_count"])
            .with_columns((pl.col("AC") / pl.col("AN")).alias("AF"))
            .with_columns(
                pl.when(pl.col("AF") < 0.5)
                .then(pl.col("AF"))
                .otherwise(1 - pl.col("AF"))
                .alias("MAF")
            )
            .sort(COORDINATES)
            .write_parquet(output[0])
        )


rule cp_dataset:
    input:
        "results/variants.annot.parquet",
    output:
        "results/dataset/test.parquet",
    shell:
        "cp {input} {output}"


rule cp_readme:
    input:
        "config/hf_readme.md",
    output:
        "results/dataset/README.md",
    shell:
        "cp {input} {output}"


rule hf_upload:
    input:
        "results/dataset/README.md",
        "results/dataset/test.parquet",
    output:
        touch("results/upload.done"),
    shell:
        "hf upload-large-folder {config[output_hf_path]} --repo-type dataset results/dataset"
